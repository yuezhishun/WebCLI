# Web CLI 分布式管理系统

这是面向 **WebCLI（Master–Slave 分布式终端管理）** 的“可落地技术方案”，进一步补齐实现细节（`node-pty + xterm-headless`、重连全量快照、服务端历史缓冲、以及 **控制面 RPC over WebSocket** 的方法清单 / 错误码 / 幂等与重试规则等）。

> 方案严格贴合已确认的需求：**PTY 创建实例、服务端维护终端网格与环形历史、断线重连推全量快照、前端不缓存状态、Master 单节点 200+ 并发连接、Slave 单节点 50+ 活跃实例、调整尺寸时可清空历史简化实现、主从 WS（二进制）与前端 WS（WSS）**等。

------

## 1) 选型结论与理由（统一口径）

### 1.1 总体选型

- **服务端（Master / Slave）**：**Fastify + ws**
- **终端子进程**：`node-pty`
- **服务端终端模拟器（无头）**：`xterm-headless`
- **前端终端渲染**：自定义渲染器（不依赖 xterm.js 的历史机制）

### 1.2 为什么这样选

- 你们所有关键链路天然是 **WebSocket + 自定义协议 + 二进制/JSON 分层**：
  - Master–Slave：WS（二进制帧）、心跳 ping/pong、控制指令 JSON、stdin/stdout 透传
  - 前端–Master：WSS，URL 携带 `instance_id` 与 JWT
- 性能与控制力优先：`ws` 更贴近标准 WS、便于实现自定义心跳/背压/二进制帧与协议演进。
- 小团队（3 人、TS 一般、1–2 周迭代）：Fastify + ws 的“薄框架”更利于把精力集中在真正难点：**终端网格/增量同步/历史缓冲/重连一致性**。
  （将来要上 NestJS，也可以把核心模块写成框架无关再迁移。）

------

## 2) 架构设计（MVP → 分布式 → SaaS 控制面）

### 2.1 MVP（单机一体）

单进程同时扮演 Master+Slave：HTTP 提供页面，WS 提供终端连接与实例管理。

### 2.2 分布式（最终目标）

- **Slave**：内网节点，持有 PTY 与终端状态（网格+历史），主动连 Master（JWT）
- **Master**：公网节点，维护 Slave 长连接，转发前端 WS 数据与控制指令
- **SaaS（控制面）**：负责认证/权限/配置模板；与 Master 交互创建实例等。

------

## 3) 关键数据流与端口

### 3.1 连接端点建议

- **前端终端数据面（WSS）**：`wss://master/ws/term?instance_id=...&token=JWT`
- **Slave 注册/数据面（WSS 或 WS+TLS）**：`wss://master/ws/slave?token=JWT`（JWT 里带 `node_id`）
- **SaaS 控制面（WSS）**：`wss://master/ws/control?token=JWT`（RPC over WS）

> 局域网简化：允许 `disable_auth=true` 关闭鉴权。

### 3.2 消息分层原则（贯穿全系统）

- **控制面**：JSON（RPC 帧、权限、实例生命周期、历史拉取、resize 等）
- **数据面**：二进制（stdin/stdout 透传）
- **终端状态同步**：优先 JSON（压缩后也可二进制），以“网格快照/增量行更新”为主，因为重连必须推“真实视觉状态”。

------

## 4) 终端实例实现（Slave）：`node-pty + xterm-headless`

### 4.1 PTY 创建与跨平台

- 使用 `node-pty` 启动子进程（命令、workdir、env、cols/rows）
- 跨平台要求：Linux/macOS/Windows（Windows 优先 ConPTY，老 Windows 可降级方案）
- 禁止依赖 tmux/screen 等外部工具。

### 4.2 无头终端模拟器（核心）

你们需求明确要求 **服务端保留完整解析引擎与缓冲区**，作为重连快照来源。
实现方式：

- 为每个实例创建一个 `xterm-headless` 的 `Terminal`（配置 cols/rows、scrollback=0 或最小）
- `pty.onData(data)` → `terminal.write(data)` 让 xterm 解析 ANSI escape、更新内部 buffer
- 从 `terminal.buffer.active` 抽取：
  - 可视区网格（rows × cols）
  - 光标位置、样式、颜色等
- **历史缓冲区（环形）**：当终端滚屏时，把被挤出的一行追加到你们自建的 ring buffer（默认 1000 行可配置）

> 为什么不直接用 xterm 的 scrollback：你们要求“前端不依赖 xterm.js 的历史回放机制”，且历史拉取由服务端按需返回。

### 4.3 终端状态结构建议（内存友好）

你们的目标是：80×25 + 1000 行历史 **<2MB/实例**，因此不要把每个 Cell 都存成大对象。

建议在内存层用“行段（runs）”编码，而不是 `[][]Cell` 真二维对象（文档描述是 Cell 网格概念，落地建议做紧凑表示）：

- `StyleId`：把 `(fg,bg,bold,italic,underline,reverse,...)` 归一化成一个小整数（查表）
- 每行存 `[{text: string, styleId: number}]` 的 segments（连续同样 style 的字符合并）
- 可视区 rows：维护 `rows` 条行段
- 历史区 ring buffer：维护最近 N 行段

这样：

- **快照**发送也天然是 segments（网络更小）
- **增量更新**常常是“整行替换”，实现简单且符合你们 MVP 指导（全行替换或差异补丁均可）

### 4.4 resize 处理（按你们文档的简化策略）

- 前端发送新 cols/rows
- Slave 调 `pty.resize(cols, rows)`（或同等 API）
- 终端模拟器重建/调整 buffer
- **清空历史缓冲区**（你们文档明确允许用此简化）

------

## 5) 终端同步协议（前端 ↔ Master ↔ Slave）

### 5.1 重连机制（强制一致性）

- 前端每次连接 WS 后，服务端立即推送 **当前全量屏幕网格快照**
- 断线重连时再次推送快照；前端不缓存任何终端状态
- 重连体验要求“强制到底部（最新内容）”

### 5.2 前端增量更新（低延迟）

你们已经允许两种增量策略：全行替换或差异补丁。建议 MVP 先用 **全行替换**（非常稳、易 debug），再做 patch 优化。

### 5.3 历史拉取（按需）

- 前端滚动超出可视区域时，向服务端请求缺失历史行范围
- 服务端从环形缓冲区返回带样式的行数据（默认容量 1000 行可配置）
- 目标性能：50 行历史响应 20ms 内

------

## 6) 控制面：RPC over WebSocket（SaaS ↔ Master）

> 你们文档当前用 gRPC proto 定义了控制面方法：Create/List/Terminate/GetWSURL/HealthCheck。
> 这里给出等价的 **RPC over WS** 规范，并补齐错误码、幂等与重试规则。

### 6.1 帧格式（JSON）

```json
// Request
{
  "v": 1,
  "type": "req",
  "id": "uuid-or-snowflake",
  "method": "CreateInstance",
  "deadline_ms": 3000,
  "idempotency_key": "optional-for-non-idempotent",
  "payload": { ... }
}

// Response
{
  "v": 1,
  "type": "resp",
  "id": "same-as-req-id",
  "ok": true,
  "payload": { ... }
}

// Error Response
{
  "v": 1,
  "type": "resp",
  "id": "same-as-req-id",
  "ok": false,
  "error": {
    "code": "INSTANCE_NOT_FOUND",
    "message": "instance_id not found",
    "retryable": false,
    "details": { }
  }
}
```

### 6.2 方法清单（控制面）

按你们 proto + 分布式需求补齐（保持最小但够用）：

1. **CreateInstance**

- `payload`：
  - `user_id`（必需）
  - `node_id`（可选；不传由 Master 选择）
  - `command`, `workdir`, `env`, `cols`, `rows`
- `resp.payload`：
  - `instance_id`
  - `websocket_url`（给前端的终端数据面 URL，含 token）

1. **ListInstances**

- `payload`：
  - `user_id`
  - `node_id?`, `status?`, `limit`, `offset`
- `resp.payload`：
  - `instances[]`, `total`

1. **TerminateInstance**

- `payload`：`instance_id`
- 行为：停止 CLI 进程、释放 PTY、前端显示 `[Process completed]`

1. **GetWebSocketURL**

- `payload`：`instance_id`, `user_id?`
- `resp.payload`：`websocket_url`（可选方法；也可 Create 时直接返回）

1. **HealthCheck**

- `payload`：空
- `resp.payload`：Master 状态、已连接 Slave 数、实例数、版本号等

（可选增强）
6) **DescribeInstance**

- 获取单实例详情（node、创建参数、状态、运行时长）

1. **ListNodes / DescribeNode**

- 提供 Master 对 Slave 的基本可见性（用于选 node_id、排障）

> 注意：**Resize / FetchHistory** 属于“终端数据面”也需要，但它们更自然放在前端↔Master 或 Master↔Slave 的控制指令里（你们文档已明确 master→slave 控制指令含 resize、拉取历史）。

------

## 7) 错误码表（控制面通用）

建议采用“稳定字符串 code + retryable”而不是只用数字，便于跨语言/跨版本演进。

| code                          | 含义              | retryable      | 典型场景                                            |
| ----------------------------- | ----------------- | -------------- | --------------------------------------------------- |
| `UNAUTHENTICATED`             | 未认证/Token 无效 | ❌              | JWT 验签失败/过期                                   |
| `PERMISSION_DENIED`           | 无权限            | ❌              | user_id 无权访问 instance（allowed_instances 策略） |
| `INVALID_ARGUMENT`            | 参数非法          | ❌              | cols/rows 非法、command 为空等                      |
| `NODE_NOT_FOUND`              | node 不存在       | ❌              | 指定 node_id 但 Master 未连接该 Slave               |
| `INSTANCE_NOT_FOUND`          | 实例不存在        | ❌              | Terminate/GetURL/Describe 传错 instance_id          |
| `INSTANCE_ALREADY_TERMINATED` | 实例已结束        | ✅(通常当作 ok) | 幂等终止                                            |
| `RESOURCE_EXHAUSTED`          | 资源不足          | ✅              | Slave 实例数超上限（目标 50+）                      |
| `UNAVAILABLE`                 | 依赖不可用        | ✅              | Slave 断开、Master 正在切换/重启                    |
| `TIMEOUT`                     | 超时              | ✅              | deadline 到期                                       |
| `CONFLICT`                    | 冲突              | ✅/❌            | 并发创建同幂等键但参数不一致（❌）                   |
| `INTERNAL`                    | 内部错误          | ✅              | 未捕获异常、协议错误                                |

------

## 8) 幂等 / 重试规则（控制面必须有）

### 8.1 幂等键（CreateInstance 必备）

**CreateInstance** 是唯一“可能产生副作用且需要重试”的关键点。

- SaaS 必须给 CreateInstance 传 `idempotency_key`
- Master 保存 `idempotency_key -> {instance_id, request_hash, created_at}`（TTL 建议 24h）
- 重复请求：
  - 若 `request_hash` 相同：直接返回同一个 `instance_id` + `websocket_url`
  - 若 `request_hash` 不同：返回 `CONFLICT`（防止幂等键被复用但参数变了）

### 8.2 其它方法的幂等性

- **TerminateInstance**：天然幂等
  - instance 不存在 → `INSTANCE_NOT_FOUND`
  - 已终止 → 返回 ok 或 `INSTANCE_ALREADY_TERMINATED`（建议 ok）
- **List / GetURL / Health**：读操作，天然幂等
- **Describe**：读操作，幂等

### 8.3 重试策略（SaaS 侧）

只对 `retryable=true` 的错误重试：

- 重试条件：`UNAVAILABLE | TIMEOUT | RESOURCE_EXHAUSTED | INTERNAL(部分)`
- 退避：指数退避 + 抖动（例如 200ms, 400ms, 800ms, 1600ms，封顶 3s）
- CreateInstance 重试必须携带相同 `idempotency_key`

------

## 9) Master–Slave 协议（控制指令 + 数据透传）

你们文档已经给出了原则：**Slave 主动连 Master（JWT），ping/pong 心跳，Master→Slave 发送 JSON 控制指令（创建/终止/resize/拉取历史），stdin/stdout 走二进制帧透传**。

落地建议：

### 9.1 Master→Slave 控制指令（JSON）

- `CreateInstance`（Master 选择 Slave 后下发）
- `TerminateInstance`
- `ResizeInstance`（来自前端的 resize 事件）
- `FetchHistory`（来自前端的历史拉取）
- `DescribeInstance`（可选）

### 9.2 数据透传（binary）

- `stdin`：前端键盘输入 → Master → Slave → pty.write
- `stdout`：pty 输出（或终端增量）→ Slave → Master → 前端

> **推荐做法**：Slave 不直接透传原始 stdout 给前端，而是“解析后生成网格增量”，这样才能满足你们“断线重连时画面必须真实一致”的核心要求，并且与“前端不缓存、重连推快照”机制一致。

------

## 10) 性能与背压（满足目标指标的关键点）

你们的最终目标指标非常明确：Slave 50+ 活跃实例、网格+1000 行历史 <2MB/实例、输入到回显 <100ms、历史拉取 50 行 <20ms、Master 200+ 并发客户端连接。

这不是“换框架就能达标”的问题，关键在策略：

### 10.1 输出节流/合并（强烈建议）

- pty 输出可能 burst 很大（比如 `top`、编译日志）
- 建议 Slave 做“按 tick 合并”（例如 16ms/33ms 一次）
  - 在 tick 内收集“被修改的行号集合”
  - tick 结束发送一个 patch：`[{y, segments}]`（全行替换）

### 10.2 每连接发送队列上限（背压）

- 对每个前端 WS 连接设置发送队列上限（字节数或消息数）
- 超限策略：
  - 丢弃中间 patch，只保留最新 patch（因为你们允许重连全量快照恢复）
  - 或触发一次 `resync`：直接推快照

### 10.3 历史缓冲区结构

- 环形 buffer（N=1000 默认，可配置）
- 行存 segments（压缩），避免存巨量 Cell 对象
- `FetchHistory` 只做 slice + 序列化，才能逼近 20ms/50 行的目标

------

## 11) 代码组织建议（让 Fastify + ws 也能长期维护）

即使不用 Nest，也建议从 Day1 把核心写成“框架无关”：

- `core/pty/`：node-pty 封装
- `core/term/`：xterm-headless 驱动、快照抽取、行段编码、diff/全行替换策略
- `core/history/`：环形 buffer
- `transport/ws/`：协议帧、RPC over WS、心跳、背压、连接管理
- `master/`：路由（instance → slave）、权限校验、SaaS 控制面
- `slave/`：实例管理（map instance_id → runtime）、资源限制

这样未来要换 NestJS，只需要替换 `http/api` 这层，核心模块不动。

------

## 12) 建议的两阶段落地里程碑（适配 1–2 周迭代）

### 第 1 个迭代（MVP 单机闭环）

对齐你们 MVP 验收：创建实例、实时输出、断线重连推快照、历史滚动拉取、resize。

### 第 2 个迭代（分布式骨架）

- Slave 主动连接 Master + JWT + 心跳 ping/pong
- Master→Slave 控制指令：Create/Terminate/Resize/FetchHistory
- Master 转发终端同步消息给前端
- 初版 SaaS 控制面：RPC over WS 的 Create/List/Terminate/GetURL/Health（等价替换 proto）

